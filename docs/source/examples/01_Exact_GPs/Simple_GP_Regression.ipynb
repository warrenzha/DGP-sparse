{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exact GP Regression Tutorial\n",
    "## Introduction\n",
    "In this notebook, we use DMGP to train a simplest regression model. We'll be modeling the function\n",
    "\\begin{align}\n",
    "&y = \\sin(2\\pi x) + \\epsilon, \\\\\n",
    "&\\epsilon \\sim \\mathcal{N}(0,0.04).\n",
    "\\end{align}\n",
    "with 100 trianing examples, and testing on 4 test examples.\n",
    "\n",
    "**Note:** this notebook is not necessarily intended to teach the mathematical background of Gaussian processes, but rather how to train a simple one and make predictions in DMGP. For a mathematical treatment, please see the reference \"A sparse expansion for deep Gaussian processes\": https://arxiv.org/abs/2112.05888"
   ],
   "id": "16df62af81e888f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:07:44.277993Z",
     "start_time": "2024-07-22T20:07:43.479600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dmgp.models import DMGP\n",
    "from dmgp.layers.activation import TMK, AMK\n",
    "from dmgp.layers.linear import LinearFlipout"
   ],
   "id": "9a118c3d696ec809",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Set up training data\n",
    "In the next cell, we set up the training data for this example. Weâ€™ll be using 100 regularly spaced points on [0,1] which we evaluate the function on and add Gaussian noise to get the training labels."
   ],
   "id": "c053f5e136deaf55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:07:44.283203Z",
     "start_time": "2024-07-22T20:07:44.279443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training data is 100 points in [0,1] inclusive regularly spaced\n",
    "train_x = torch.linspace(0, 1, 100)\n",
    "train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2\n",
    "\n",
    "class RegressionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "dataset = RegressionDataset(train_x, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=20, shuffle=True)"
   ],
   "id": "742b0fe8b2eb1b82",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setting up the model\n",
    "The next cell demonstrates the most critical features of a user-defined DGP model in DMGP. Basically, there are two ways to define a customized DGP model: use `dmgp.models.DMGP` or construct your model from scratch using `dmgp.layers`.\n",
    "\n",
    "If we define the model using `dmgp.models.DMGP`, you need to construct the following DMGP objects:\n",
    "1. Input and output dimension (`input_dim`, `output_dim`) - For regression `output_dim=1`.\n",
    "2. Number of hidden layers (`num_layers`) - Usually we use 2 hidden layers.\n",
    "3. Level of inducing (`num_inducing`) - This sets the level of inducing points we use for GP inference.\n",
    "4. Dimension of hidden layers (`hidden_dim`) - This sets the width of hidden layers.\n",
    "5. A kernel (`kernel`) - This defines the prior covariance of the GP. (In DMGP, `DMGP.kernels.laplace_kernel.LaplaceProductKernel` is a good choice to start).\n",
    "6. A layer type (`layer_type`) - This defines the linear Bayesian layer we use for learnable GP weights. (`DMGP.layers.linear.LinearFlipout` is used by default)."
   ],
   "id": "59c9d0c728996cf4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:07:44.291414Z",
     "start_time": "2024-07-22T20:07:44.284288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dmgp.utils.sparse_design.design_class import HyperbolicCrossDesign\n",
    "from dmgp.kernels.laplace_kernel import LaplaceProductKernel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using: \", device)\n",
    "\n",
    "# Define the model using DMGP\n",
    "model = DMGP(input_dim=1,\n",
    "             output_dim=1,\n",
    "             num_layers=2,\n",
    "             hidden_dim=8,\n",
    "             num_inducing=3,\n",
    "             input_lb=0,\n",
    "             input_ub=1,\n",
    "             kernel=LaplaceProductKernel(1.),\n",
    "             design_class=HyperbolicCrossDesign,\n",
    "             layer_type=LinearFlipout,\n",
    "             option='additive').to(device)"
   ],
   "id": "128700eaafb008db",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If we define the customized model from scratch, the following is an example which consists of two GP layers, each with level-8 and level-3 sparse grid design, respectively.",
   "id": "2e5d43495927353f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:07:44.318446Z",
     "start_time": "2024-07-22T20:07:44.292405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a DMGP model from scratch\n",
    "class DMGP_regression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, design_class, kernel):\n",
    "        super(DMGP_regression, self).__init__()\n",
    "        \n",
    "        # 1st layer of DGP: input:[n, input_dim] size tensor, output:[n, 8] size tensor\n",
    "        self.mk1 = AMK(in_features=input_dim, n_level=8, input_lb=0, input_ub=1, design_class=design_class, kernel=kernel)\n",
    "        self.fc1 = LinearFlipout(self.mk1.out_features, 16)\n",
    "\n",
    "        # 2nd layer of DGP: input:[n, 8] size tensor, output:[n, output_dim] size tensor\n",
    "        self.mk2 = AMK(in_features=16, n_level=3, input_lb=0, input_ub=1, design_class=design_class, kernel=kernel)\n",
    "        self.fc2 = LinearFlipout(in_features=self.mk2.out_features, out_features=output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        kl_sum = 0\n",
    "        x = self.mk1(x)\n",
    "        x, kl = self.fc1(x)\n",
    "        kl_sum += kl\n",
    "        x = self.mk2(x)\n",
    "        x, kl = self.fc2(x)\n",
    "        kl_sum += kl\n",
    "        return torch.squeeze(x), kl_sum\n",
    "    \n",
    "model = DMGP_regression(input_dim=1, output_dim=1, design_class=HyperbolicCrossDesign, kernel=LaplaceProductKernel(1.)).to(device)"
   ],
   "id": "326494b2f5445aaf",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training the model\n",
    "In the next cell, we handle using variational inference (VI) to train the parameters of the DMGP."
   ],
   "id": "20376675cf48bb24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:07:50.291307Z",
     "start_time": "2024-07-22T20:07:44.320275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.unsqueeze(-1).to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output_ = []\n",
    "        kl_ = []\n",
    "        for mc_run in range(5):\n",
    "            output, kl = model(data)\n",
    "            output_.append(output)\n",
    "            kl_.append(kl)\n",
    "        output = torch.mean(torch.stack(output_), dim=0)\n",
    "        kl = torch.mean(torch.stack(kl_), dim=0)\n",
    "        nll_loss = F.mse_loss(output, target)\n",
    "        # ELBO loss\n",
    "        loss = nll_loss + (kl / 20)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
   ],
   "id": "6183472f7f7d7d88",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Making predictions with the model\n",
    "In the next cell, we make predictions with the model. To do this, we simply put the model in eval mode, and call both modules on the test data."
   ],
   "id": "e3fac42806bb0ec1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:07:50.306877Z",
     "start_time": "2024-07-22T20:07:50.292232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test points are regularly spaced along [0,1]\n",
    "test_x = torch.linspace(0, 1, 4)\n",
    "test_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2\n",
    "\n",
    "test_set = RegressionDataset(test_x, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=50, shuffle=True)\n",
    "\n",
    "# Make predictions by feeding test dat through DMGP model\n",
    "test_loss = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.unsqueeze(-1).to(device), target.to(device)\n",
    "        predicts = []\n",
    "        for mc_run in range(20):\n",
    "            model.eval()\n",
    "            output, _ = model.forward(data)\n",
    "            loss = F.mse_loss(output, target).cpu().data.numpy()\n",
    "            test_loss.append(loss)\n",
    "            predicts.append(output.cpu().data.numpy())\n",
    "\n",
    "        pred_mean = np.mean(predicts, axis=0)\n",
    "        pred_std = np.std(predicts, axis=0)\n",
    "        \n",
    "print('test loss: ', np.mean(test_loss))"
   ],
   "id": "92f1d2b8e102221a",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Plot the model fit\n",
    "In the next cell, we plot the mean and confidence region of the DMGP prediction."
   ],
   "id": "50b57dd6edd7f560"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:07:51.012170Z",
     "start_time": "2024-07-22T20:07:50.307767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Confidence level\n",
    "confidence_level = 0.95\n",
    "z = stats.norm.ppf(1 - (1 - confidence_level) / 2)  # z-score for 95% confidence\n",
    "\n",
    "# Calculate the confidence interval\n",
    "lower = pred_mean - z * pred_std\n",
    "upper = pred_mean + z * pred_std\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.numpy(), pred_mean, 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.numpy(), lower, upper, alpha=0.5)\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ],
   "id": "808a38aa9d455808",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:07:51.056335Z",
     "start_time": "2024-07-22T20:07:51.021792Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e6691986be8459b4",
   "execution_count": 7,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
