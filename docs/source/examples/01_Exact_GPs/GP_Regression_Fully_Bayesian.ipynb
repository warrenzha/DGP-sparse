{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fully Bayesian GPs\n",
    "In this notebook, weâ€™ll demonstrate how to integrate DMGP and MCMC to sample GP parameters and perform GP inference in a fully Bayesian way."
   ],
   "id": "4b55bc72fcde1354"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T21:08:39.900693Z",
     "start_time": "2024-07-22T21:08:39.016787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from dmgp.models import DMGP\n",
    "from dmgp.layers.linear import LinearFlipout"
   ],
   "id": "26dd72de839841d7",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T21:08:39.905458Z",
     "start_time": "2024-07-22T21:08:39.901840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training data is 1000 points randomly selected by Normal distribution\n",
    "inputs = np.random.random((1000, 2))\n",
    "outputs = 1 / (1 + np.exp(np.sum(inputs, axis=-1)))\n",
    "inputs = inputs.astype(np.float32)\n",
    "outputs = np.squeeze(outputs).astype(np.float32)\n",
    "\n",
    "class RegressionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "dataset = RegressionDataset(inputs, outputs)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)"
   ],
   "id": "cd391cc7e63c0ad5",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T21:08:39.912910Z",
     "start_time": "2024-07-22T21:08:39.906090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dmgp.utils.sparse_design.design_class import HyperbolicCrossDesign\n",
    "from dmgp.kernels.laplace_kernel import LaplaceProductKernel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using: \", device)\n",
    "\n",
    "# Define the model using DMGP\n",
    "model = DMGP(input_dim=2,\n",
    "             output_dim=1,\n",
    "             num_layers=2,\n",
    "             hidden_dim=8,\n",
    "             num_inducing=3,\n",
    "             input_lb=-2,\n",
    "             input_ub=2,\n",
    "             kernel=LaplaceProductKernel(1.),\n",
    "             design_class=HyperbolicCrossDesign,\n",
    "             layer_type=LinearFlipout,\n",
    "             option='additive').to(device)"
   ],
   "id": "9fbab8fb8d89eb1f",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training the model\n",
    "In the next cell, we handle using variational inference (VI) to train the parameters of the DMGP. During each step, we use MCMC to sample the log-likelihood for ELBO loss."
   ],
   "id": "76f849d5dc6f52c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T21:08:39.916863Z",
     "start_time": "2024-07-22T21:08:39.913802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, optimizer, train_loader, num_mc=5):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        target = target.to(device)\n",
    "        data = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output_ = []\n",
    "        kl_ = []\n",
    "        for mc_run in range(num_mc):  # Run MCMC to sample log-likelihood\n",
    "            output, kl = model(data)\n",
    "            output_.append(output)\n",
    "            kl_.append(kl)\n",
    "        output = torch.mean(torch.stack(output_), dim=0)\n",
    "        kl = torch.mean(torch.stack(kl_), dim=0)\n",
    "        nll_loss = F.mse_loss(output, target)\n",
    "        # ELBO loss\n",
    "        loss = nll_loss + (kl / 64)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses"
   ],
   "id": "3c43abded9c2cb64",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T21:08:57.982469Z",
     "start_time": "2024-07-22T21:08:39.918464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=1.0)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.999)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "    loss = train(model, optimizer, train_loader, num_mc=5)\n",
    "    scheduler.step()\n",
    "    losses += loss\n",
    "        \n",
    "plt.plot(losses)"
   ],
   "id": "d5336e3e004c8f88",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Running Sampling\n",
    "In the next cell, we evaluate the model by loading the samples generated by MCMC. This converts `model` from a single DGP to a batch of `num_monte_carlo` DGPs, in this case 20."
   ],
   "id": "e693d37d4e4a0e73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T21:08:57.987097Z",
     "start_time": "2024-07-22T21:08:57.983624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model, test_loader, num_monte_carlo=20):\n",
    "    test_loss = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            target = target.to(device)\n",
    "            data = data.to(device)\n",
    "\n",
    "            predicts = []\n",
    "            for mc_run in range(num_monte_carlo):\n",
    "                model.eval()\n",
    "                output, _ = model(data)\n",
    "                loss = F.mse_loss(output, target).cpu().data.numpy()\n",
    "                test_loss.append(loss)\n",
    "                predicts.append(output.cpu().data.numpy())\n",
    "\n",
    "            pred_mean = np.mean(predicts, axis=0)\n",
    "            pred_var = np.var(predicts, axis=0)\n",
    "\n",
    "            # print('prediction mean: ', pred_mean, 'prediction var: ', pred_var)\n",
    "\n",
    "        print('test loss: ', np.mean(test_loss))"
   ],
   "id": "e0f9f42bd9bb74b4",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T21:08:58.313161Z",
     "start_time": "2024-07-22T21:08:57.988170Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate(model, test_loader)",
   "id": "b3bb02fea5881e56",
   "execution_count": 7,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
