{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Defining a custom example model in DMGP\n",
    "In this notebook, we are looking at how to define a custom model in DMGP. As an example, we consider a 2-layer sparse DGP model for a regression task. The model consists of two layers, each with a level-3 sparse grid design."
   ],
   "id": "1b1e2221e44c7cac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T21:28:34.283201Z",
     "start_time": "2024-07-22T21:28:33.475585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dmgp.layers.linear import LinearFlipout\n",
    "from dmgp.layers.activation import TMK"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Defining a 2-layer DMGP Model\n",
    "\n",
    "In the next cell, we define our simple exact DMGP for regression. The model consists of two layers, each with level-3 sparse grid design. "
   ],
   "id": "7767bd7948758eb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T21:28:34.568199Z",
     "start_time": "2024-07-22T21:28:34.284296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dmgp.utils.sparse_design.design_class import HyperbolicCrossDesign\n",
    "from dmgp.kernels.laplace_kernel import LaplaceProductKernel\n",
    "\n",
    "# Define a 2-layer DMGP model for regression\n",
    "class DMGP_regression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, design_class, kernel):\n",
    "        super(DMGP_regression, self).__init__()\n",
    "        \n",
    "        # 1st layer of DGP: input:[n, input_dim] size tensor, output:[n, 8] size tensor\n",
    "        self.tmk1 = TMK(in_features=input_dim, n_level=3, design_class=design_class, kernel=kernel)\n",
    "        self.fc1 = LinearFlipout(self.tmk1.out_features, 7)\n",
    "\n",
    "        # 2nd layer of DGP: input:[n, 8] size tensor, output:[n, output_dim] size tensor\n",
    "        self.tmk2 = TMK(in_features=7, n_level=3, design_class=design_class, kernel=kernel)\n",
    "        self.fc2 = LinearFlipout(in_features=self.tmk2.out_features, out_features=output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        kl_sum = 0\n",
    "        x = self.tmk1(x)\n",
    "        x, kl = self.fc1(x)\n",
    "        kl_sum += kl\n",
    "        x = self.tmk2(x)\n",
    "        x, kl = self.fc2(x)\n",
    "        kl_sum += kl\n",
    "        return torch.squeeze(x), kl_sum\n",
    "    \n",
    "model = DMGP_regression(input_dim=1, \n",
    "                        output_dim=1, \n",
    "                        design_class=HyperbolicCrossDesign, \n",
    "                        kernel=LaplaceProductKernel(1.))"
   ],
   "id": "5f19e34aab5634b8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Viewing model hyperparameters\n",
    "Let's take a look at the model parameters. By \"parameters\", here I mean explicitly objects of type `torch.nn.Parameter` that will have gradients filled in by autograd. To access these, we use `model.state_dict()` which returns a dictionary of the model's parameters."
   ],
   "id": "26693bfcc7ef2595"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T21:28:34.570876Z",
     "start_time": "2024-07-22T21:28:34.568992Z"
    }
   },
   "cell_type": "code",
   "source": "model_params = model.state_dict()",
   "id": "37bb1477a9ac8694",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Counting the number of parameters\n",
    "We can count the total number of parameters in the model by counting the number of parameters in each layer."
   ],
   "id": "28541c16e649131"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T21:28:34.576115Z",
     "start_time": "2024-07-22T21:28:34.571765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parameter_count(model):\n",
    "    all_parameters = list(model.parameters())\n",
    "    layer_parameters=[len(i) for i in all_parameters]\n",
    "    print(layer_parameters)\n",
    "    print(\"Total number of parameters in the network: \", sum(layer_parameters))\n",
    "    return sum(layer_parameters)\n",
    "\n",
    "parameter_count(model)"
   ],
   "id": "5cc00477ad42c55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 7, 7, 7, 1, 1, 1, 1]\n",
      "Total number of parameters in the network:  32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Viewing model architecture\n",
    "We can also print the model architecture by simply printing the model object."
   ],
   "id": "fd71e97c684843a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T21:28:34.579251Z",
     "start_time": "2024-07-22T21:28:34.577614Z"
    }
   },
   "cell_type": "code",
   "source": "print(model)",
   "id": "87662ff606111482",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMGP_regression(\n",
      "  (tmk1): TMK(\n",
      "    (kernel): LaplaceProductKernel()\n",
      "  )\n",
      "  (fc1): LinearFlipout()\n",
      "  (tmk2): TMK(\n",
      "    (kernel): LaplaceProductKernel()\n",
      "  )\n",
      "  (fc2): LinearFlipout()\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Saving Model State\n",
    "The state dictionary above represents all trainable parameters in the model. We can save this state dictionary to a file and load it back later."
   ],
   "id": "9067fbe791f59fbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T21:28:34.584027Z",
     "start_time": "2024-07-22T21:28:34.579900Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), 'model_state.pth')",
   "id": "eb48a81cc48b70e6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loading Model State\n",
    "Next, we load this state in to a new model and demonstrate that the parameters were updated correctly."
   ],
   "id": "707a8dc6953f7f5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T21:28:34.852883Z",
     "start_time": "2024-07-22T21:28:34.584684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "state_dict = torch.load('model_state.pth')\n",
    "model = DMGP_regression(input_dim=1, \n",
    "                        output_dim=1, \n",
    "                        design_class=HyperbolicCrossDesign, \n",
    "                        kernel=LaplaceProductKernel(1.))\n",
    "model.load_state_dict(state_dict)"
   ],
   "id": "c2335f37d55188c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T21:28:34.863498Z",
     "start_time": "2024-07-22T21:28:34.853717Z"
    }
   },
   "cell_type": "code",
   "source": "model.state_dict()",
   "id": "73ce26622507dcec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('tmk1.design_points',\n",
       "              tensor([[ 0.0000],\n",
       "                      [-1.0000],\n",
       "                      [ 1.0000],\n",
       "                      [-1.5000],\n",
       "                      [-0.5000],\n",
       "                      [ 0.5000],\n",
       "                      [ 1.5000]])),\n",
       "             ('tmk1.chol_inv',\n",
       "              tensor(indices=tensor([[0, 0, 1, 0, 2, 1, 3, 1, 4, 0, 0, 5, 2, 2, 6, 0, 0, 0, 0],\n",
       "                                     [0, 1, 1, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 0, 0, 0, 0]]),\n",
       "                     values=tensor([ 1.0000, -0.3956,  1.0754, -0.3956,  1.0754, -0.7629,\n",
       "                                     1.2578, -0.6523,  1.4710, -0.6523, -0.6523,  1.4710,\n",
       "                                    -0.6523, -0.7629,  1.2578,  0.0000,  0.0000,  0.0000,\n",
       "                                     0.0000]),\n",
       "                     size=(7, 7), nnz=19, layout=torch.sparse_coo)),\n",
       "             ('fc1.mu_weight',\n",
       "              tensor([[-0.0758,  0.0461, -0.1482,  0.0683, -0.1386, -0.1030,  0.0920],\n",
       "                      [-0.0433, -0.0989, -0.1573,  0.0411, -0.1746, -0.1865, -0.0910],\n",
       "                      [ 0.1376,  0.1809,  0.0703, -0.0856, -0.0096, -0.0557, -0.0680],\n",
       "                      [ 0.0087, -0.0800,  0.0533,  0.0043,  0.0915,  0.0007,  0.1087],\n",
       "                      [ 0.0138, -0.1435,  0.2128,  0.0849,  0.0640, -0.0250,  0.0250],\n",
       "                      [ 0.0242, -0.0251, -0.1028,  0.2220, -0.1137, -0.0019, -0.0104],\n",
       "                      [-0.0174, -0.1707,  0.1792,  0.0341, -0.0407, -0.0649, -0.0362]])),\n",
       "             ('fc1.rho_weight',\n",
       "              tensor([[-2.9610, -2.8409, -3.0827, -3.0614, -2.9089, -2.9239, -3.1900],\n",
       "                      [-2.9692, -3.1854, -3.1357, -3.1254, -3.0065, -3.0056, -2.9677],\n",
       "                      [-3.2493, -2.8507, -3.0081, -3.0514, -2.9805, -2.9939, -2.9325],\n",
       "                      [-2.9661, -3.1350, -2.9952, -3.0927, -3.1206, -2.9105, -3.0006],\n",
       "                      [-3.1013, -2.8592, -3.0712, -2.8221, -3.1352, -2.9597, -3.0522],\n",
       "                      [-2.8918, -2.8943, -3.0316, -3.0251, -3.0586, -3.1681, -2.9983],\n",
       "                      [-2.9478, -2.8540, -2.9983, -2.7958, -2.8706, -3.0605, -3.2139]])),\n",
       "             ('fc1.mu_bias',\n",
       "              tensor([-0.0011,  0.0394,  0.0660, -0.0557, -0.0698,  0.0396, -0.0157])),\n",
       "             ('fc1.rho_bias',\n",
       "              tensor([-2.9976, -2.9965, -2.9239, -2.9051, -2.8914, -3.1101, -2.9702])),\n",
       "             ('tmk2.design_points',\n",
       "              tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                      [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -1.0000],\n",
       "                      [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
       "                      ...,\n",
       "                      [ 0.7500,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                      [ 1.2500,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                      [ 1.7500,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])),\n",
       "             ('tmk2.chol_inv',\n",
       "              tensor(indices=tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
       "                                     [ 0,  1, 15,  ...,  0,  0,  0]]),\n",
       "                     values=tensor([ 1.0000e+00, -3.9563e-01, -2.1458e-06,  ...,\n",
       "                                     0.0000e+00,  0.0000e+00,  0.0000e+00]),\n",
       "                     size=(799, 799), nnz=15435, layout=torch.sparse_coo)),\n",
       "             ('fc2.mu_weight',\n",
       "              tensor([[-9.0566e-02, -2.8624e-02,  4.2705e-02, -1.2480e-01,  2.4715e-01,\n",
       "                        1.0615e-01, -9.0250e-02, -7.5183e-02, -1.5594e-01,  2.8537e-02,\n",
       "                       -1.2841e-01,  1.3783e-01,  3.8319e-02,  1.5348e-01, -3.1485e-02,\n",
       "                        1.2187e-01,  2.5971e-02,  2.7458e-02, -6.4424e-02, -1.0684e-01,\n",
       "                       -9.4449e-02, -6.4293e-02, -7.0036e-03,  2.0621e-01, -3.8708e-02,\n",
       "                       -8.6008e-02, -2.6089e-02, -1.4771e-01, -1.5390e-01,  1.0283e-01,\n",
       "                       -3.2298e-02, -1.7455e-02, -1.8675e-01,  8.1489e-02,  1.1372e-01,\n",
       "                        1.0783e-02,  4.5066e-02,  8.0364e-02,  3.9583e-03,  1.8171e-01,\n",
       "                        3.2521e-02, -1.7447e-01,  3.3643e-02, -1.2403e-01, -2.6304e-02,\n",
       "                        1.6667e-02,  9.8904e-02,  2.9614e-02, -5.2247e-02,  1.5516e-01,\n",
       "                       -9.1938e-02,  9.9696e-02, -9.4554e-02,  2.7562e-02,  6.4433e-02,\n",
       "                        1.2263e-01,  1.9740e-01, -1.2540e-02,  9.5585e-02, -6.3377e-02,\n",
       "                       -8.3855e-02, -1.5696e-01,  1.8577e-01, -5.5056e-02, -1.0327e-02,\n",
       "                        9.4896e-03,  4.3368e-02, -5.1866e-02,  9.3177e-02,  1.7929e-01,\n",
       "                        1.4771e-01,  6.3214e-02, -2.6150e-01,  1.2188e-02,  1.1394e-01,\n",
       "                       -6.6903e-02,  3.3828e-02,  4.2603e-02,  3.3877e-02, -1.0643e-01,\n",
       "                       -3.6609e-03, -1.5210e-02,  1.5906e-01, -1.5029e-01, -3.8120e-02,\n",
       "                        4.1300e-02, -6.3063e-02, -9.8755e-02,  9.1866e-02,  8.4531e-02,\n",
       "                        8.9520e-02, -4.1472e-02, -6.1710e-02,  9.5331e-02, -1.2257e-01,\n",
       "                        6.7282e-02,  3.1236e-02,  6.8449e-02,  6.1660e-02, -1.4078e-01,\n",
       "                        5.8610e-02,  1.6282e-02,  8.3734e-02, -1.5553e-01, -4.1298e-02,\n",
       "                       -1.3570e-01, -9.7179e-02,  1.0777e-01, -1.5364e-01,  2.3183e-02,\n",
       "                        3.9795e-02, -7.7380e-02,  2.5401e-02, -1.0611e-01,  5.4711e-02,\n",
       "                        1.0087e-01, -1.9993e-01, -4.8786e-02, -9.0732e-03,  2.8141e-02,\n",
       "                       -1.8834e-02, -3.3546e-02,  1.0465e-01,  1.1593e-01,  5.8564e-02,\n",
       "                       -6.4076e-02, -4.3520e-02,  1.1870e-02,  3.6949e-02, -3.0089e-02,\n",
       "                       -7.2017e-03,  1.0783e-01, -1.3566e-01,  1.5861e-02, -1.7562e-01,\n",
       "                       -7.0496e-02,  3.5462e-02,  6.1253e-02,  1.0570e-01,  7.1237e-02,\n",
       "                        4.4819e-02, -1.6266e-01, -2.6502e-02, -1.3763e-01,  9.1344e-02,\n",
       "                        9.7436e-03,  2.1976e-01,  1.1714e-01,  8.8300e-03, -1.8933e-01,\n",
       "                       -1.7427e-01,  4.3810e-02, -4.7856e-02, -4.2920e-02, -1.1734e-01,\n",
       "                        3.4334e-02,  1.5853e-01,  7.8417e-02,  1.6646e-02, -4.4478e-02,\n",
       "                        8.7208e-02,  1.3779e-02,  1.1826e-01, -4.1931e-02,  6.3037e-02,\n",
       "                        2.3699e-01, -9.7081e-02,  3.1736e-02, -1.3823e-01, -5.7340e-02,\n",
       "                       -6.6042e-02,  7.5207e-02, -2.6185e-02, -1.6552e-01,  8.0952e-03,\n",
       "                       -3.1667e-02, -1.2857e-01, -2.5005e-02,  1.6637e-01, -5.8774e-02,\n",
       "                       -1.3650e-01,  6.9728e-02,  1.1253e-01, -7.0361e-02,  1.0863e-02,\n",
       "                        9.3067e-02, -1.2443e-01,  7.6777e-02,  5.2680e-02,  1.0739e-01,\n",
       "                        1.0844e-01, -1.4292e-01,  1.0951e-02, -7.3202e-02, -1.2833e-02,\n",
       "                        2.2884e-02,  7.3628e-02, -1.5291e-01,  5.6790e-02,  2.3541e-02,\n",
       "                       -2.3739e-01,  1.0450e-01, -1.9494e-01, -2.0275e-01,  2.5260e-01,\n",
       "                        9.7383e-02, -6.5200e-02,  9.8746e-02,  1.1100e-01,  1.5517e-01,\n",
       "                        1.0391e-01,  1.9841e-01, -1.3994e-01, -1.3880e-01,  7.9772e-03,\n",
       "                       -3.0878e-02, -1.2827e-01, -3.3147e-02,  6.1775e-02,  6.9955e-02,\n",
       "                        2.2005e-01,  1.6216e-01,  2.2223e-02, -6.0456e-02, -7.5258e-02,\n",
       "                        3.2969e-02, -2.3695e-02, -6.2828e-02, -1.2174e-01, -5.1532e-02,\n",
       "                       -4.0442e-02, -2.1850e-01, -4.8300e-02,  2.2549e-01, -9.8796e-02,\n",
       "                       -9.2801e-02,  8.5777e-02,  4.9842e-02, -7.9084e-02,  7.5676e-02,\n",
       "                       -1.5436e-01,  7.0912e-02, -1.4359e-01,  1.1453e-02,  2.1214e-01,\n",
       "                        1.1002e-01, -1.1294e-01,  8.9085e-03,  7.9175e-02,  6.8099e-02,\n",
       "                        1.1282e-01,  1.0326e-01,  1.1010e-01,  5.4082e-02, -6.9224e-02,\n",
       "                       -2.3135e-01, -1.8750e-01, -9.9721e-02,  1.2523e-01, -1.0577e-01,\n",
       "                        4.3748e-03, -3.0816e-02, -8.4776e-02, -3.0370e-02, -2.7781e-03,\n",
       "                        1.3862e-03,  1.6292e-01,  1.7716e-02, -2.1340e-02,  1.3357e-01,\n",
       "                       -1.3107e-01, -1.1598e-01, -1.1303e-01, -2.1644e-02, -8.3053e-02,\n",
       "                        6.3997e-02, -1.1729e-01, -1.2448e-01,  1.8617e-01, -2.8340e-02,\n",
       "                        5.9220e-02,  1.1756e-02, -1.6013e-02,  8.4374e-02, -6.4682e-02,\n",
       "                       -8.1419e-02,  1.2055e-02,  2.6535e-02, -4.7584e-02,  8.3760e-02,\n",
       "                        1.2612e-01, -7.1600e-02,  5.0169e-02,  1.6194e-01, -6.4124e-02,\n",
       "                        3.2601e-02,  2.1675e-01,  1.1291e-01, -7.8362e-02,  6.3380e-02,\n",
       "                       -1.0005e-01, -6.9040e-03, -3.0298e-02,  7.6945e-02, -5.7385e-02,\n",
       "                       -2.4227e-02,  1.2897e-01,  4.3846e-03, -1.8637e-01,  1.0308e-02,\n",
       "                       -1.1458e-01,  2.0985e-01,  2.2494e-02,  1.2386e-01, -1.7956e-01,\n",
       "                       -8.8500e-02,  1.1840e-01, -2.3861e-02,  1.0737e-01,  1.4122e-01,\n",
       "                       -3.6972e-02, -8.0350e-03,  3.4169e-02, -1.1516e-01, -8.7882e-02,\n",
       "                        1.1479e-01,  2.4553e-02, -5.9298e-02,  6.3440e-02,  1.4616e-01,\n",
       "                        1.6661e-01, -1.8955e-01, -2.5194e-02, -1.9373e-01,  2.7025e-02,\n",
       "                        2.2613e-02,  1.1656e-02, -2.8478e-03, -1.2411e-02, -1.9626e-02,\n",
       "                       -6.0706e-02, -8.1967e-02,  7.6944e-02,  2.9775e-02, -8.6981e-02,\n",
       "                       -1.0051e-01,  3.8290e-02,  2.1566e-04, -2.6668e-03,  7.2234e-02,\n",
       "                        5.9257e-03,  4.1066e-02,  1.3627e-01, -1.2867e-01,  7.2041e-02,\n",
       "                        4.8902e-02, -3.1023e-02,  1.1042e-02, -2.3930e-02, -2.0194e-01,\n",
       "                        1.2960e-02, -1.4769e-03,  4.7053e-02, -2.2880e-01,  1.3477e-01,\n",
       "                        7.6579e-02, -2.3823e-02,  2.2108e-02, -5.7295e-02, -5.3630e-02,\n",
       "                        6.0889e-02, -6.4551e-02, -2.9543e-01,  9.8297e-02, -2.0940e-01,\n",
       "                       -4.5366e-02, -2.0839e-01, -5.0602e-02, -4.8639e-02, -1.5378e-01,\n",
       "                       -2.6429e-02,  1.4261e-01, -3.4932e-02,  3.0645e-02, -1.4780e-01,\n",
       "                       -1.9791e-02, -1.8106e-01, -2.3285e-02,  4.9180e-02,  9.3478e-03,\n",
       "                       -2.3006e-02,  3.5312e-02,  5.8503e-02,  3.2732e-02,  8.8672e-02,\n",
       "                        1.8793e-01, -4.4826e-02, -9.3557e-02, -5.8099e-02,  7.3177e-02,\n",
       "                       -2.6507e-02, -7.8945e-04, -1.4230e-01, -2.9917e-02, -9.6510e-02,\n",
       "                       -1.0163e-01,  4.7213e-03, -9.5203e-02,  8.3273e-02,  1.2649e-02,\n",
       "                       -1.9526e-01, -1.4638e-01, -1.5524e-01,  1.4681e-03,  3.7206e-03,\n",
       "                        1.2112e-01,  3.5450e-02,  8.1843e-02,  9.7532e-02, -1.3723e-01,\n",
       "                       -1.4676e-01, -7.9534e-02,  6.7593e-02,  8.1212e-02, -6.3246e-02,\n",
       "                        2.0440e-02,  3.3684e-02,  1.0552e-01, -1.3181e-03,  1.1899e-02,\n",
       "                       -2.4617e-01, -7.7224e-02, -2.3910e-01, -9.8123e-03, -3.0493e-02,\n",
       "                       -3.4826e-02,  4.5093e-03, -1.7785e-01,  9.0977e-02,  5.0355e-03,\n",
       "                       -3.7190e-03, -7.7366e-02,  6.5385e-02, -4.0313e-02,  1.2627e-01,\n",
       "                       -1.6123e-01,  5.1043e-04, -6.0412e-02,  1.2601e-01,  1.3982e-01,\n",
       "                       -2.4296e-03, -6.3719e-02,  5.2087e-02, -2.7474e-03,  1.0171e-01,\n",
       "                        1.6224e-02, -6.1818e-02,  5.7848e-02,  1.0099e-01, -1.9292e-01,\n",
       "                       -4.6238e-02, -3.9743e-02, -1.6590e-01, -3.6371e-02, -7.7644e-02,\n",
       "                       -3.6467e-02, -1.4779e-01,  4.6508e-02,  5.7638e-02,  8.0970e-02,\n",
       "                       -2.4874e-02,  6.9467e-02, -1.0427e-02,  8.4325e-02,  2.4016e-02,\n",
       "                       -1.3484e-01, -9.8959e-02,  6.9999e-02,  2.3095e-03,  6.5888e-02,\n",
       "                       -1.2195e-01,  5.8114e-02,  2.3980e-02,  1.1989e-01, -4.7478e-03,\n",
       "                       -5.2683e-03, -2.3547e-02,  8.7672e-03, -1.4323e-01, -8.7442e-03,\n",
       "                       -4.3970e-02, -3.5427e-02, -1.5954e-01, -4.2270e-02, -4.3322e-03,\n",
       "                       -5.3996e-02, -2.1753e-03,  4.0799e-02, -1.6387e-01, -1.1661e-01,\n",
       "                       -9.1687e-02, -1.8367e-02,  7.9346e-02, -7.8718e-02,  5.0260e-02,\n",
       "                        1.9573e-02, -1.8096e-01, -1.6014e-01, -9.9276e-02,  9.5236e-02,\n",
       "                       -9.7995e-02,  2.5043e-02,  1.5060e-01, -1.4534e-01, -2.9986e-03,\n",
       "                       -1.7564e-02,  8.1177e-02, -7.4269e-02,  4.6129e-02,  5.6558e-02,\n",
       "                        3.5237e-02, -1.6133e-02,  1.4658e-01, -6.6028e-03, -4.2619e-02,\n",
       "                        1.4006e-01, -7.7288e-02,  9.4876e-02,  8.2283e-03, -1.9333e-02,\n",
       "                       -6.7366e-02, -2.1154e-01,  4.1521e-02,  2.8471e-02, -5.2754e-02,\n",
       "                        6.9474e-03, -1.2548e-01,  2.4062e-02,  4.5108e-02,  1.2674e-01,\n",
       "                        1.4459e-01,  1.2157e-01, -4.6629e-02,  8.8361e-02, -2.4156e-02,\n",
       "                        3.1700e-02, -9.5958e-02, -2.0407e-01,  4.2660e-02,  5.0596e-02,\n",
       "                       -1.2419e-01, -1.1000e-01, -8.8035e-03,  1.8074e-02, -1.3248e-03,\n",
       "                       -7.7761e-02,  1.6068e-01,  2.2984e-02,  1.5350e-02,  1.4651e-01,\n",
       "                       -3.1382e-02,  7.8014e-02,  1.7814e-01,  1.1980e-01, -2.1880e-01,\n",
       "                        4.3000e-02,  1.0277e-01, -6.3166e-02, -1.7306e-02,  5.0552e-02,\n",
       "                       -5.0921e-02, -5.6328e-02,  1.3134e-01,  1.1664e-01, -5.0420e-02,\n",
       "                        1.0835e-01, -3.2904e-02,  2.0538e-01, -9.3104e-02,  1.8604e-01,\n",
       "                        9.9586e-03, -9.9408e-02,  1.2404e-01, -1.8087e-01,  8.2243e-02,\n",
       "                       -1.5902e-01,  8.3467e-02,  1.1503e-01, -1.0159e-01, -8.7861e-02,\n",
       "                        1.5680e-01,  2.5090e-01, -1.6133e-01, -1.4494e-01, -5.9941e-02,\n",
       "                        4.5024e-02, -9.9805e-02, -9.9807e-02,  2.3449e-01,  1.7902e-01,\n",
       "                       -1.8434e-03, -2.4997e-02,  6.5381e-02, -1.1526e-01, -1.9298e-02,\n",
       "                        1.0178e-01, -2.3129e-03, -1.7760e-01,  1.4898e-02,  3.4325e-02,\n",
       "                        5.9894e-02,  6.1419e-02,  1.5583e-01,  8.0671e-02, -1.0987e-01,\n",
       "                       -2.1315e-02,  7.9107e-02,  1.1011e-01, -9.0022e-02,  1.7554e-01,\n",
       "                       -1.1984e-01, -1.7706e-02, -1.7851e-01,  1.7043e-01, -3.4221e-02,\n",
       "                        5.8850e-02, -2.1809e-01, -1.7087e-01,  2.7423e-02,  5.9244e-02,\n",
       "                       -3.7688e-02,  1.8659e-02, -7.8346e-02,  1.2154e-01, -1.2785e-01,\n",
       "                        1.2447e-02, -1.7022e-02, -8.9890e-02, -1.7995e-01, -8.7991e-02,\n",
       "                        1.0733e-01,  7.9852e-02,  9.1625e-02, -4.1645e-02,  1.9944e-02,\n",
       "                        7.3423e-02, -2.1423e-01, -1.8541e-02, -5.3181e-02,  1.8012e-02,\n",
       "                       -1.4968e-02, -9.1133e-02,  8.2482e-02,  8.1706e-02, -1.1264e-01,\n",
       "                        6.4404e-02, -2.0223e-01,  7.3134e-02, -8.6328e-02, -3.3123e-02,\n",
       "                       -2.2015e-01,  4.3762e-02, -4.8341e-02,  2.1976e-02, -1.7406e-02,\n",
       "                        4.4597e-02, -1.0716e-01,  1.3763e-02, -1.5461e-02,  9.2397e-02,\n",
       "                        4.8375e-02,  1.2883e-01, -1.3944e-02, -4.9523e-02,  5.3047e-02,\n",
       "                       -1.2089e-02,  1.6298e-02,  3.6046e-02,  9.8596e-02, -9.9437e-02,\n",
       "                       -9.5868e-02,  5.0883e-02, -9.5117e-02, -1.9352e-02, -2.0299e-02,\n",
       "                       -9.9696e-03,  5.0477e-02,  1.4723e-01, -2.4316e-01, -3.9712e-02,\n",
       "                       -2.0745e-01,  4.6693e-02,  2.3327e-01,  9.2612e-03, -4.6967e-02,\n",
       "                       -2.8125e-02,  1.2468e-01, -7.6221e-02, -6.6385e-02, -2.8407e-02,\n",
       "                       -8.6991e-02, -3.8917e-03,  4.7416e-02, -1.8904e-01, -5.9097e-02,\n",
       "                        2.3460e-02,  2.3008e-02, -2.5665e-02,  4.2593e-02, -1.0546e-01,\n",
       "                       -8.7923e-02, -2.1268e-02, -9.4328e-02, -6.4748e-02,  7.6578e-02,\n",
       "                       -1.9466e-02,  8.5756e-02,  8.4182e-02, -9.9101e-02,  1.1470e-02,\n",
       "                       -9.5328e-02, -4.0348e-02, -1.5099e-01, -1.9920e-01, -7.7994e-02,\n",
       "                        1.0907e-01, -2.3922e-03, -1.7416e-02,  9.8486e-02, -6.7835e-02,\n",
       "                       -1.3933e-02,  6.9998e-02, -1.6639e-01,  2.5990e-02, -1.0688e-02,\n",
       "                       -1.1758e-02, -3.4174e-02, -1.0838e-01, -6.8503e-02,  6.3551e-02,\n",
       "                       -2.2048e-02,  1.0049e-01,  4.2181e-02,  2.0727e-01,  1.1708e-01,\n",
       "                       -1.0361e-01, -5.4346e-02,  1.0435e-02, -5.7873e-02, -2.5202e-02,\n",
       "                        8.1543e-02,  3.6074e-02, -3.4486e-02,  9.7248e-02,  6.1158e-03,\n",
       "                        1.5619e-01, -6.8589e-02, -5.7826e-02,  1.9502e-01, -2.2819e-01,\n",
       "                        2.3836e-02, -1.2636e-01, -3.4811e-02,  8.4863e-02, -2.7415e-01,\n",
       "                        4.0684e-02,  1.6142e-01, -4.2412e-02,  2.1588e-02,  7.2438e-02,\n",
       "                        1.7112e-01, -1.0560e-01, -1.0615e-01,  1.2316e-01,  1.1318e-01,\n",
       "                       -2.1903e-03, -9.9657e-03, -1.0772e-01,  3.2921e-02,  1.6353e-01,\n",
       "                       -1.4030e-01,  4.6654e-02, -5.7236e-02, -2.8390e-01, -1.2510e-01,\n",
       "                       -6.4874e-03, -1.2114e-01, -1.8772e-02,  5.6933e-02,  1.0263e-01,\n",
       "                       -2.5037e-02,  1.7766e-02,  1.3400e-01, -1.4787e-01, -1.8878e-01,\n",
       "                        9.0701e-03, -3.7014e-02,  3.7418e-02, -4.4218e-02]])),\n",
       "             ('fc2.rho_weight',\n",
       "              tensor([[-3.2977, -3.1178, -3.0840, -2.8176, -3.0311, -3.2271, -3.0551, -2.9747,\n",
       "                       -2.8730, -3.0327, -3.0579, -2.9532, -3.1056, -3.0482, -3.0912, -3.1498,\n",
       "                       -3.0913, -2.9875, -2.8883, -2.9146, -3.0673, -2.9551, -3.1228, -3.0894,\n",
       "                       -2.9945, -3.1387, -3.0516, -2.9695, -2.9477, -2.9642, -3.0896, -3.0789,\n",
       "                       -2.9439, -3.1303, -2.9208, -2.9126, -2.8192, -2.8907, -3.0365, -2.9597,\n",
       "                       -2.8332, -3.1142, -3.0794, -3.0352, -3.0854, -3.1320, -3.1990, -2.9307,\n",
       "                       -2.9292, -3.0358, -2.9763, -3.1346, -2.9874, -2.9359, -2.9949, -3.0042,\n",
       "                       -2.9063, -3.0492, -3.1444, -3.0563, -2.8235, -3.1139, -2.8272, -3.0962,\n",
       "                       -3.0094, -2.9325, -2.9924, -2.9802, -2.8021, -2.8244, -3.0105, -2.9266,\n",
       "                       -3.0979, -3.0837, -3.0155, -2.9062, -3.0790, -2.9145, -3.0535, -3.1744,\n",
       "                       -2.9930, -3.0449, -2.9644, -2.9981, -2.9020, -2.8674, -2.9301, -3.0550,\n",
       "                       -3.0141, -3.1406, -3.1312, -3.0183, -3.2700, -3.0437, -3.0580, -2.9642,\n",
       "                       -2.9722, -3.1145, -2.9764, -2.9097, -2.9521, -2.8784, -2.9776, -2.9016,\n",
       "                       -3.0780, -3.0796, -2.8534, -3.0516, -2.9897, -2.9321, -3.0314, -3.0819,\n",
       "                       -3.0047, -2.9872, -2.8876, -2.9744, -3.0603, -3.0695, -3.0433, -3.1131,\n",
       "                       -3.0240, -2.9301, -3.0029, -2.9480, -2.9160, -3.1555, -3.1457, -2.9629,\n",
       "                       -2.9929, -2.9115, -2.9625, -2.9797, -3.0422, -3.0491, -3.0109, -3.1031,\n",
       "                       -2.9857, -2.7965, -3.1710, -2.9748, -2.9502, -2.8464, -3.0146, -3.1064,\n",
       "                       -2.9977, -3.1281, -3.1384, -3.1286, -2.9542, -3.0951, -2.8168, -2.8690,\n",
       "                       -3.1065, -3.0403, -2.8694, -3.1145, -3.0824, -2.9769, -3.1029, -2.8640,\n",
       "                       -2.7940, -2.8104, -3.0960, -3.0465, -3.0160, -2.8170, -2.9569, -2.9043,\n",
       "                       -2.7658, -2.8766, -3.0726, -2.7851, -3.0769, -2.9465, -3.2672, -2.8773,\n",
       "                       -2.9170, -2.9547, -3.0225, -3.1365, -2.8333, -3.1247, -3.0722, -3.0780,\n",
       "                       -2.8481, -3.2133, -2.9083, -3.0777, -3.1834, -2.8455, -2.9133, -3.0330,\n",
       "                       -3.0641, -2.9555, -3.0004, -2.8956, -2.9599, -3.0283, -2.8404, -3.1284,\n",
       "                       -3.0366, -2.9174, -3.0954, -2.9886, -3.1612, -3.0701, -2.9264, -2.9687,\n",
       "                       -3.0675, -2.8764, -3.0689, -2.9413, -3.2106, -3.0606, -3.0488, -3.1605,\n",
       "                       -2.8662, -2.8051, -2.7862, -2.8252, -2.9223, -3.0217, -2.9347, -3.0924,\n",
       "                       -3.1415, -2.9723, -2.9201, -2.9500, -3.0832, -3.0988, -3.0409, -2.9844,\n",
       "                       -2.8945, -3.0011, -3.0594, -3.1123, -3.0836, -3.0630, -2.9173, -3.2532,\n",
       "                       -2.8973, -2.9906, -3.0159, -3.0317, -3.1571, -2.9558, -3.1942, -3.0675,\n",
       "                       -2.9727, -3.0132, -3.0103, -2.9628, -3.1350, -3.1173, -3.0032, -2.9802,\n",
       "                       -3.1215, -2.9847, -3.0236, -3.0818, -3.1416, -3.0364, -3.1698, -2.9989,\n",
       "                       -2.8696, -2.9606, -3.1145, -3.1761, -3.0284, -3.0113, -2.9897, -2.8260,\n",
       "                       -2.8782, -2.8950, -2.9595, -2.8615, -3.0735, -2.9121, -3.1632, -2.7958,\n",
       "                       -2.9025, -2.9875, -3.1191, -3.0743, -3.0043, -2.8905, -3.0172, -2.9202,\n",
       "                       -3.1074, -3.0594, -3.1009, -2.8730, -3.0682, -3.0951, -3.1296, -2.8899,\n",
       "                       -2.9865, -2.9320, -3.0491, -3.0267, -3.0204, -2.9537, -3.0559, -2.8373,\n",
       "                       -2.9709, -2.9968, -2.9882, -3.1900, -2.9433, -2.9970, -3.0458, -3.0036,\n",
       "                       -2.9757, -2.9996, -2.9103, -2.9500, -3.0939, -2.8487, -3.0588, -3.1038,\n",
       "                       -2.9605, -2.8919, -2.9273, -2.9371, -2.9866, -2.8487, -3.0854, -3.0669,\n",
       "                       -2.9349, -2.9850, -2.9873, -2.9593, -2.8384, -3.1188, -2.9968, -2.9115,\n",
       "                       -3.1895, -2.9819, -3.0472, -2.9442, -2.9833, -2.9923, -3.0718, -2.9598,\n",
       "                       -3.0805, -3.0647, -2.9490, -2.8536, -3.0383, -3.1416, -3.0475, -2.8396,\n",
       "                       -2.9521, -3.1425, -3.1670, -3.0042, -3.1056, -2.9485, -3.0199, -3.0051,\n",
       "                       -3.0304, -3.1758, -3.1256, -3.0784, -3.1732, -3.0433, -3.1032, -3.1906,\n",
       "                       -3.1379, -2.8105, -2.9468, -2.8504, -3.0917, -2.8797, -2.9739, -2.8029,\n",
       "                       -3.1201, -2.9135, -2.9863, -3.0082, -3.0556, -2.9226, -2.9109, -3.0582,\n",
       "                       -2.9736, -2.8769, -3.1490, -2.9986, -3.1218, -2.9152, -2.8295, -3.0780,\n",
       "                       -2.9653, -2.9823, -3.0678, -3.1002, -3.2105, -2.9547, -2.9929, -3.0025,\n",
       "                       -3.0129, -2.8503, -3.1686, -3.1033, -3.0013, -3.0958, -2.9842, -3.0933,\n",
       "                       -3.0736, -2.9543, -3.1274, -3.0678, -2.8752, -3.1159, -3.0713, -2.8964,\n",
       "                       -2.9501, -2.8748, -2.9783, -3.1483, -3.1715, -3.1046, -3.0731, -2.8074,\n",
       "                       -3.0795, -3.0664, -2.8302, -3.0966, -2.9496, -3.0739, -2.9845, -2.9747,\n",
       "                       -2.9409, -2.7416, -3.1480, -3.0915, -3.1116, -2.9114, -2.9723, -2.9838,\n",
       "                       -3.0763, -2.7471, -2.9683, -2.9841, -3.1493, -3.1271, -2.9554, -2.8434,\n",
       "                       -2.9629, -2.8930, -3.0788, -2.9794, -3.2104, -2.9718, -2.9613, -2.8266,\n",
       "                       -2.9905, -3.0028, -2.8691, -2.9784, -3.1319, -2.8193, -3.0762, -3.1257,\n",
       "                       -3.0427, -3.0629, -2.9052, -3.1463, -2.9910, -2.9004, -2.9604, -2.8647,\n",
       "                       -3.0352, -3.0037, -3.0073, -2.9305, -2.8114, -3.1076, -2.9839, -3.0082,\n",
       "                       -3.0047, -2.9555, -3.0828, -2.9431, -3.0718, -2.8963, -3.1441, -2.9447,\n",
       "                       -3.0735, -2.8457, -2.8905, -2.9695, -3.0884, -3.0937, -3.0710, -3.0959,\n",
       "                       -2.9823, -2.8729, -2.8473, -3.2192, -2.9622, -2.9969, -2.8953, -3.0639,\n",
       "                       -2.9962, -2.9356, -2.9086, -3.2354, -3.0174, -2.9552, -2.8652, -2.9972,\n",
       "                       -3.0075, -2.8635, -3.0997, -2.9262, -3.0064, -3.0714, -2.9543, -2.9004,\n",
       "                       -3.1034, -3.0442, -3.2455, -3.0804, -3.0084, -2.9631, -2.9230, -3.1032,\n",
       "                       -3.1021, -3.0131, -3.0437, -2.8509, -3.0254, -3.0521, -3.0735, -2.9327,\n",
       "                       -2.9275, -3.0782, -2.8734, -3.0054, -3.0319, -3.0834, -2.9655, -2.9063,\n",
       "                       -3.0809, -2.8733, -3.0665, -3.0169, -3.0006, -3.0257, -2.8919, -2.9058,\n",
       "                       -3.0145, -2.9028, -3.0994, -3.1199, -2.9866, -3.0743, -2.9882, -2.9150,\n",
       "                       -3.1875, -2.9235, -2.9552, -3.1840, -2.9914, -3.1294, -2.9973, -2.8366,\n",
       "                       -2.8814, -3.1026, -2.8845, -3.0913, -3.1841, -3.0789, -2.8917, -2.9937,\n",
       "                       -2.9596, -3.0509, -3.1410, -3.0805, -2.9138, -2.9783, -2.8416, -3.0249,\n",
       "                       -2.8948, -2.9132, -2.9807, -2.8519, -2.9958, -3.0719, -2.9667, -2.9310,\n",
       "                       -2.9080, -3.1384, -2.9684, -3.1345, -3.0947, -2.8450, -2.8873, -2.9626,\n",
       "                       -3.0477, -3.1167, -3.1327, -3.0384, -2.9597, -3.0925, -2.9328, -3.0191,\n",
       "                       -2.8998, -3.1117, -3.0831, -2.9544, -2.9981, -3.1496, -3.0863, -2.8850,\n",
       "                       -3.1159, -2.9083, -2.9460, -3.0088, -3.0529, -3.1378, -3.0903, -3.0737,\n",
       "                       -2.9374, -2.9881, -3.0225, -2.8873, -3.0373, -2.8509, -2.9993, -2.9235,\n",
       "                       -3.0715, -2.8165, -2.9751, -3.1786, -2.9750, -3.0151, -3.0938, -2.9173,\n",
       "                       -3.0494, -2.8524, -2.9578, -3.0468, -2.7433, -3.1904, -3.2800, -3.0187,\n",
       "                       -3.1046, -2.9246, -3.1248, -2.9823, -2.8556, -2.8549, -3.0354, -2.9485,\n",
       "                       -3.0737, -2.8070, -2.8615, -3.0270, -3.0720, -3.1414, -3.0737, -2.8999,\n",
       "                       -2.8981, -3.1642, -3.0531, -3.0110, -3.1745, -3.0054, -3.0600, -2.9978,\n",
       "                       -3.1235, -3.0303, -2.9070, -2.8580, -3.0216, -3.0586, -2.9848, -3.1741,\n",
       "                       -3.1001, -2.8783, -2.9927, -2.9965, -2.9501, -2.9180, -3.0593, -2.9998,\n",
       "                       -2.8387, -2.7561, -3.0820, -2.8574, -3.0354, -3.0475, -3.2092, -2.9588,\n",
       "                       -3.0485, -2.8716, -3.0708, -2.9639, -3.1091, -3.0186, -3.0259, -3.0026,\n",
       "                       -2.9424, -2.9851, -3.0662, -3.0498, -3.2107, -2.8129, -3.0825, -3.0623,\n",
       "                       -2.9261, -2.9475, -3.1372, -3.0487, -3.0103, -3.2173, -3.0535, -3.2881,\n",
       "                       -2.9902, -3.0234, -2.9684, -2.9472, -3.1022, -2.9041, -2.8667, -3.1139,\n",
       "                       -2.9313, -3.1510, -3.0215, -3.0472, -2.9248, -2.9818, -2.9943, -3.1348,\n",
       "                       -2.8242, -2.9726, -2.9544, -2.9783, -2.9562, -2.9099, -2.9994, -3.0108,\n",
       "                       -2.8914, -2.9451, -3.0765, -2.9007, -3.0665, -3.0508, -3.0119, -2.8556,\n",
       "                       -3.1584, -2.8856, -2.9629, -3.2397, -3.0318, -3.1327, -3.1308, -3.0242,\n",
       "                       -3.0249, -3.1120, -2.8873, -3.0565, -2.9620, -3.0592, -2.9848, -2.9821,\n",
       "                       -2.9436, -3.0892, -3.0634, -3.1313, -2.9127, -2.9218, -3.0204, -3.1631,\n",
       "                       -2.8683, -2.8751, -3.1853, -3.1557, -2.9694, -3.0710, -2.9346, -3.0594,\n",
       "                       -2.9026, -3.0549, -2.9207, -2.9549, -2.8475, -3.0271, -2.9467, -2.9854,\n",
       "                       -3.2400, -2.8819, -3.0807, -2.9184, -2.8892, -3.0709, -3.0075]])),\n",
       "             ('fc2.mu_bias', tensor([-0.0632])),\n",
       "             ('fc2.rho_bias', tensor([-3.0214]))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "See our [documentation](https://sparse-dgp.readthedocs.io/en/latest/) for more information on how to use the library.",
   "id": "1ca660770bc27323"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
